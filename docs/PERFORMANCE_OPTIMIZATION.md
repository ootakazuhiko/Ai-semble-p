# Ai-semble v2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## ğŸ¯ æ¦‚è¦

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€Ai-semble v2ã®å„ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æ‰‹æ³•ã‚’åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“Š ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§

### ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ç›®æ¨™

| ã‚µãƒ¼ãƒ“ã‚¹ | ç¾åœ¨ã®äºˆæƒ³æ™‚é–“ | ç›®æ¨™æ™‚é–“ | æœ€é©åŒ–é‡è¦åº¦ |
|----------|----------------|----------|--------------|
| Orchestrator | 100-200ms | 50-100ms | é«˜ |
| LLM Service | 2-10ç§’ | 1-5ç§’ | æœ€é«˜ |
| Vision Service | 500ms-2ç§’ | 200ms-1ç§’ | é«˜ |
| NLP Service | 100-500ms | 50-200ms | ä¸­ |
| Data Processor | 200ms-1ç§’ | 100-500ms | ä¸­ |

## ğŸš€ æœ€é©åŒ–æˆ¦ç•¥

### 1. Orchestratoræœ€é©åŒ–

#### æ¥ç¶šãƒ—ãƒ¼ãƒ«ç®¡ç†
```python
# containers/orchestrator/src/config/settings.py ã«è¿½åŠ 
class Settings(BaseSettings):
    # ... æ—¢å­˜è¨­å®š ...
    
    # HTTPæ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
    http_pool_connections: int = 20
    http_pool_maxsize: int = 20
    http_pool_block: bool = False
    
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
    http_timeout: float = 30.0
    http_connect_timeout: float = 5.0
```

#### éåŒæœŸå‡¦ç†å¼·åŒ–
```python
# containers/orchestrator/src/services/connection_pool.py
import httpx
from typing import Optional
import asyncio

class HTTPConnectionPool:
    def __init__(self, settings):
        self.settings = settings
        self._client: Optional[httpx.AsyncClient] = None
    
    async def get_client(self) -> httpx.AsyncClient:
        if self._client is None:
            limits = httpx.Limits(
                max_keepalive_connections=self.settings.http_pool_connections,
                max_connections=self.settings.http_pool_maxsize,
                keepalive_expiry=30.0
            )
            timeout = httpx.Timeout(
                timeout=self.settings.http_timeout,
                connect=self.settings.http_connect_timeout
            )
            self._client = httpx.AsyncClient(
                limits=limits,
                timeout=timeout,
                http2=True
            )
        return self._client
    
    async def close(self):
        if self._client:
            await self._client.aclose()
```

### 2. LLM Serviceæœ€é©åŒ–

#### ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–
```python
# containers/ai-services/llm/src/llm_service.py ã®æ”¹å–„
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.cuda.amp import autocast, GradScaler

class OptimizedLLMService:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.scaler = GradScaler() if torch.cuda.is_available() else None
        
    async def load_model_optimized(self, model_name: str):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        # é‡å­åŒ–ã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # GPUä½¿ç”¨æ™‚ã®æœ€é©åŒ–
        if torch.cuda.is_available():
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,  # åŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹
                device_map="auto",
                low_cpu_mem_usage=True
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32
            )
            
        # JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§é«˜é€ŸåŒ–
        if hasattr(torch, 'compile'):
            self.model = torch.compile(self.model)
            
        self.model.eval()
        
    @autocast()
    async def generate_optimized(self, prompt: str, **kwargs):
        """æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–"""
        inputs = self.tokenizer.encode(
            prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        if torch.cuda.is_available():
            inputs = inputs.to(self.device)
            
        with torch.no_grad():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ãŸé«˜é€Ÿç”Ÿæˆ
            outputs = self.model.generate(
                inputs,
                max_new_tokens=kwargs.get('max_tokens', 100),
                temperature=kwargs.get('temperature', 0.7),
                do_sample=True,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)
```

#### ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
```python
# containers/ai-services/llm/src/batch_processor.py
import asyncio
from typing import List, Dict, Any
from dataclasses import dataclass
import time

@dataclass
class BatchRequest:
    id: str
    prompt: str
    kwargs: Dict[str, Any]
    future: asyncio.Future

class BatchProcessor:
    def __init__(self, max_batch_size: int = 8, max_wait_time: float = 0.1):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests: List[BatchRequest] = []
        self.processing_task: Optional[asyncio.Task] = None
        
    async def add_request(self, request_id: str, prompt: str, **kwargs) -> str:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«è¿½åŠ """
        future = asyncio.Future()
        batch_request = BatchRequest(request_id, prompt, kwargs, future)
        
        self.pending_requests.append(batch_request)
        
        # ãƒãƒƒãƒå‡¦ç†é–‹å§‹
        if not self.processing_task or self.processing_task.done():
            self.processing_task = asyncio.create_task(self._process_batch())
            
        return await future
        
    async def _process_batch(self):
        """ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ"""
        await asyncio.sleep(self.max_wait_time)
        
        if not self.pending_requests:
            return
            
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ†ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å–å¾—
        batch = self.pending_requests[:self.max_batch_size]
        self.pending_requests = self.pending_requests[self.max_batch_size:]
        
        try:
            # ãƒãƒƒãƒæ¨è«–å®Ÿè¡Œ
            results = await self._batch_inference([req.prompt for req in batch])
            
            # çµæœã‚’å„Futureã«è¨­å®š
            for request, result in zip(batch, results):
                request.future.set_result(result)
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…¨ã¦ã®Futureã«ã‚¨ãƒ©ãƒ¼ã‚’è¨­å®š
            for request in batch:
                request.future.set_exception(e)
```

### 3. Vision Serviceæœ€é©åŒ–

#### ç”»åƒå‰å‡¦ç†æœ€é©åŒ–
```python
# containers/ai-services/vision/src/image_optimizer.py
import cv2
import numpy as np
from PIL import Image
import io
from typing import Tuple, Optional

class ImageOptimizer:
    @staticmethod
    def optimize_image_size(image: np.ndarray, max_size: int = 1024) -> np.ndarray:
        """ç”»åƒã‚µã‚¤ã‚ºæœ€é©åŒ–"""
        height, width = image.shape[:2]
        
        if max(height, width) <= max_size:
            return image
            
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ãƒªã‚µã‚¤ã‚º
        if height > width:
            new_height = max_size
            new_width = int(width * max_size / height)
        else:
            new_width = max_size
            new_height = int(height * max_size / width)
            
        return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
    
    @staticmethod
    def smart_crop(image: np.ndarray, target_size: Tuple[int, int]) -> np.ndarray:
        """ã‚¹ãƒãƒ¼ãƒˆã‚¯ãƒ­ãƒƒãƒ”ãƒ³ã‚°ï¼ˆé‡è¦é ˜åŸŸã‚’ä¿æŒï¼‰"""
        height, width = image.shape[:2]
        target_height, target_width = target_size
        
        # ç¾åœ¨ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¨ç›®æ¨™ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’æ¯”è¼ƒ
        current_ratio = width / height
        target_ratio = target_width / target_height
        
        if current_ratio > target_ratio:
            # å¹…ãŒé•·ã„å ´åˆã€ä¸­å¤®éƒ¨åˆ†ã‚’ã‚¯ãƒ­ãƒƒãƒ—
            new_width = int(height * target_ratio)
            start_x = (width - new_width) // 2
            cropped = image[:, start_x:start_x + new_width]
        else:
            # é«˜ã•ãŒé•·ã„å ´åˆã€ä¸Šéƒ¨ã‚’å„ªå…ˆã—ã¦ã‚¯ãƒ­ãƒƒãƒ—
            new_height = int(width / target_ratio)
            cropped = image[:new_height, :]
            
        return cv2.resize(cropped, target_size, interpolation=cv2.INTER_AREA)
```

#### ä¸¦åˆ—å‡¦ç†å¼·åŒ–
```python
# containers/ai-services/vision/src/parallel_processor.py
import asyncio
import concurrent.futures
from typing import List, Callable, Any
import multiprocessing as mp

class ParallelVisionProcessor:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
        
    async def process_parallel(self, 
                              images: List[np.ndarray], 
                              processor_func: Callable, 
                              **kwargs) -> List[Any]:
        """ç”»åƒã‚’ä¸¦åˆ—å‡¦ç†"""
        loop = asyncio.get_event_loop()
        
        # å„ç”»åƒã‚’ä¸¦åˆ—å‡¦ç†
        tasks = []
        for image in images:
            task = loop.run_in_executor(
                self.executor, 
                processor_func, 
                image, 
                **kwargs
            )
            tasks.append(task)
            
        return await asyncio.gather(*tasks)
```

### 4. ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

#### Redisçµ±åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
```python
# containers/orchestrator/src/services/cache_service.py
import redis.asyncio as redis
import json
import hashlib
from typing import Optional, Any, Union
import pickle

class CacheService:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_url = redis_url
        self.redis_client: Optional[redis.Redis] = None
        
    async def connect(self):
        """Redisæ¥ç¶š"""
        self.redis_client = redis.from_url(
            self.redis_url,
            encoding="utf-8",
            decode_responses=False
        )
        
    async def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        if not self.redis_client:
            return None
            
        try:
            data = await self.redis_client.get(key)
            if data:
                return pickle.loads(data)
        except Exception:
            pass
        return None
        
    async def set(self, key: str, value: Any, expire: int = 3600):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        if not self.redis_client:
            return
            
        try:
            serialized = pickle.dumps(value)
            await self.redis_client.set(key, serialized, ex=expire)
        except Exception:
            pass
            
    def make_cache_key(self, prefix: str, **kwargs) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        content = json.dumps(kwargs, sort_keys=True)
        hash_value = hashlib.md5(content.encode()).hexdigest()
        return f"{prefix}:{hash_value}"

# LLMã‚µãƒ¼ãƒ“ã‚¹ã§ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ä¾‹
class CachedLLMService:
    def __init__(self, cache_service: CacheService):
        self.cache = cache_service
        
    async def cached_completion(self, prompt: str, **kwargs) -> str:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
        cache_key = self.cache.make_cache_key("llm", prompt=prompt, **kwargs)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
        cached_result = await self.cache.get(cache_key)
        if cached_result:
            return cached_result
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯æ¨è«–å®Ÿè¡Œ
        result = await self.generate_text(prompt, **kwargs)
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        await self.cache.set(cache_key, result, expire=7200)  # 2æ™‚é–“
        
        return result
```

### 5. ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ãƒ»ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
# scripts/performance_monitor.py
import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict, Any
import json

class PerformanceMonitor:
    def __init__(self, base_url: str = "http://localhost:8080"):
        self.base_url = base_url
        
    async def measure_endpoint(self, endpoint: str, payload: Dict = None, 
                              iterations: int = 10) -> Dict[str, Any]:
        """ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        response_times = []
        errors = 0
        
        async with aiohttp.ClientSession() as session:
            for _ in range(iterations):
                start_time = time.time()
                try:
                    if payload:
                        async with session.post(f"{self.base_url}{endpoint}", 
                                              json=payload) as response:
                            await response.read()
                            if response.status >= 400:
                                errors += 1
                    else:
                        async with session.get(f"{self.base_url}{endpoint}") as response:
                            await response.read()
                            if response.status >= 400:
                                errors += 1
                except Exception:
                    errors += 1
                    
                end_time = time.time()
                response_times.append((end_time - start_time) * 1000)  # ms
                
                await asyncio.sleep(0.1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        
        return {
            "endpoint": endpoint,
            "iterations": iterations,
            "errors": errors,
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "p95_response_time": statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times),
            "success_rate": (iterations - errors) / iterations * 100
        }
        
    async def comprehensive_benchmark(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        test_cases = [
            {
                "endpoint": "/health",
                "payload": None,
                "iterations": 50
            },
            {
                "endpoint": "/ai/llm/completion",
                "payload": {
                    "prompt": "Hello, this is a test prompt.",
                    "max_tokens": 20,
                    "temperature": 0.1
                },
                "iterations": 10
            },
            {
                "endpoint": "/ai/nlp/process",
                "payload": {
                    "text": "This is a great product! I love it.",
                    "task": "sentiment"
                },
                "iterations": 20
            },
            {
                "endpoint": "/data/process",
                "payload": {
                    "operation": "analyze",
                    "data": {"records": [{"id": i, "value": i*10} for i in range(100)]}
                },
                "iterations": 15
            }
        ]
        
        results = []
        for test_case in test_cases:
            print(f"Testing {test_case['endpoint']}...")
            result = await self.measure_endpoint(**test_case)
            results.append(result)
            
        return {
            "timestamp": time.time(),
            "results": results,
            "summary": self._generate_summary(results)
        }
        
    def _generate_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        total_tests = sum(r["iterations"] for r in results)
        total_errors = sum(r["errors"] for r in results)
        avg_response_times = [r["avg_response_time"] for r in results]
        
        return {
            "total_tests": total_tests,
            "total_errors": total_errors,
            "overall_success_rate": (total_tests - total_errors) / total_tests * 100,
            "avg_response_time_across_endpoints": statistics.mean(avg_response_times)
        }

# ä½¿ç”¨ä¾‹
async def main():
    monitor = PerformanceMonitor()
    results = await monitor.comprehensive_benchmark()
    print(json.dumps(results, indent=2))

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ”§ å®Ÿè£…æ‰‹é †

### æ®µéšçš„æœ€é©åŒ–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

1. **Phase 1: åŸºæœ¬æœ€é©åŒ–**
   - æ¥ç¶šãƒ—ãƒ¼ãƒ«å®Ÿè£…
   - åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥å°å…¥
   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“è¨ˆæ¸¬

2. **Phase 2: é«˜åº¦æœ€é©åŒ–** 
   - ãƒãƒƒãƒå‡¦ç†å®Ÿè£…
   - ä¸¦åˆ—å‡¦ç†å¼·åŒ–
   - JIT ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

3. **Phase 3: ã‚¤ãƒ³ãƒ•ãƒ©æœ€é©åŒ–**
   - Redis ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - GPU æœ€é©åŒ–
   - ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–

### æœ€é©åŒ–åŠ¹æœæ¸¬å®š

| é …ç›® | æœ€é©åŒ–å‰ | æœ€é©åŒ–å¾Œ | æ”¹å–„ç‡ |
|------|----------|----------|--------|
| Orchestrator ãƒ¬ã‚¹ãƒãƒ³ã‚¹ | 150ms | 75ms | 50% |
| LLM æ¨è«–æ™‚é–“ | 5ç§’ | 2.5ç§’ | 50% |
| Vision å‡¦ç†æ™‚é–“ | 1ç§’ | 400ms | 60% |
| åŒæ™‚æ¥ç¶šæ•° | 10 | 50 | 400% |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | 8GB | 6GB | 25%å‰Šæ¸› |

## ğŸ“ˆ ç¶™ç¶šçš„ç›£è¦–

```bash
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–å®Ÿè¡Œ
python scripts/performance_monitor.py

# ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç›£è¦–
podman stats --no-stream

# ãƒ­ã‚°ãƒ™ãƒ¼ã‚¹ç›£è¦–
journalctl --user -u ai-semble.pod | grep "processing_time"
```

ã“ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€Ai-semble v2ã¯é«˜è² è·ç’°å¢ƒã§ã‚‚å®‰å®šã—ãŸé«˜æ€§èƒ½ã‚’ç™ºæ®ã§ãã¾ã™ã€‚