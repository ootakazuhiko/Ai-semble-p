# Ai-semble v2 APIä½¿ç”¨ä¾‹é›†

## ğŸ“‹ ç›®æ¬¡

1. [åŸºæœ¬çš„ãªAPIå‘¼ã³å‡ºã—](#åŸºæœ¬çš„ãªapiå‘¼ã³å‡ºã—)
2. [å®Ÿç”¨çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹](#å®Ÿç”¨çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹)
3. [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°](#ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
4. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–)
5. [SDKãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ä¾‹](#sdkãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ä¾‹)

## ğŸš€ åŸºæœ¬çš„ãªAPIå‘¼ã³å‡ºã—

### ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

```bash
# åŸºæœ¬çš„ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl -X GET http://localhost:8080/health

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹
{
  "status": "healthy",
  "timestamp": 1701234567.89,
  "service": "orchestrator", 
  "version": "2.0.0"
}
```

### LLMæ¨è«–

```bash
# ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
curl -X POST http://localhost:8080/ai/llm/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "AIã¨ã¯ä½•ã‹ã‚’100æ–‡å­—ä»¥å†…ã§èª¬æ˜ã—ã¦ãã ã•ã„",
    "max_tokens": 150,
    "temperature": 0.7
  }'

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹
{
  "job_id": "llm-20241205-001",
  "status": "completed",
  "result": "AIã¯äººå·¥çŸ¥èƒ½ã®ã“ã¨ã§ã€äººé–“ã®çŸ¥çš„ãªæ´»å‹•ã‚’ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§æ¨¡å€£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ©Ÿæ¢°å­¦ç¿’ã‚„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ´»ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—å•é¡Œè§£æ±ºã‚„äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚",
  "tokens_used": 87
}
```

### ãƒ‡ãƒ¼ã‚¿å‡¦ç†

```bash
# ãƒ‡ãƒ¼ã‚¿åˆ†æ
curl -X POST http://localhost:8080/data/process \
  -H "Content-Type: application/json" \
  -d '{
    "operation": "analyze",
    "data": {
      "records": [
        {"name": "Alice", "age": 25, "score": 85},
        {"name": "Bob", "age": 30, "score": 92},
        {"name": "Carol", "age": 28, "score": 78}
      ]
    }
  }'

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹
{
  "status": "completed",
  "result": {
    "shape": [3, 3],
    "numeric_summary": {
      "age": {"mean": 27.67, "std": 2.52},
      "score": {"mean": 85.0, "std": 7.0}
    },
    "missing_values": {"age": 0, "score": 0}
  },
  "rows_processed": 3,
  "processing_time": 0.15
}
```

## ğŸ’¼ å®Ÿç”¨çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

### 1. é¡§å®¢ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```bash
#!/bin/bash
# feedback_analysis.sh

BASE_URL="http://localhost:8080"

# Step 1: ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
echo "1. ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."
UPLOAD_RESPONSE=$(curl -s -X POST $BASE_URL/data/upload \
  -F "file=@customer_feedback.csv")
echo $UPLOAD_RESPONSE

# Step 2: åŸºæœ¬çµ±è¨ˆåˆ†æ
echo "2. åŸºæœ¬çµ±è¨ˆåˆ†æå®Ÿè¡Œä¸­..."
ANALYSIS_RESPONSE=$(curl -s -X POST $BASE_URL/data/process \
  -H "Content-Type: application/json" \
  -d '{
    "operation": "analyze",
    "data": {"file_path": "/data/customer_feedback.csv"}
  }')
echo $ANALYSIS_RESPONSE

# Step 3: æ„Ÿæƒ…åˆ†æï¼ˆLLMä½¿ç”¨ï¼‰
echo "3. æ„Ÿæƒ…åˆ†æå®Ÿè¡Œä¸­..."
SENTIMENT_RESPONSE=$(curl -s -X POST $BASE_URL/ai/llm/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ä»¥ä¸‹ã®é¡§å®¢ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®æ„Ÿæƒ…å‚¾å‘ã‚’åˆ†æã—ã€ä¸»è¦ãªä¸æº€ç‚¹ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„:\n[ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ã“ã“ã«æŒ¿å…¥]",
    "max_tokens": 300,
    "temperature": 0.3
  }')
echo $SENTIMENT_RESPONSE

# Step 4: æ”¹å–„ææ¡ˆç”Ÿæˆ
echo "4. æ”¹å–„ææ¡ˆç”Ÿæˆä¸­..."
SUGGESTION_RESPONSE=$(curl -s -X POST $BASE_URL/ai/llm/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "é¡§å®¢ã®ä¸»è¦ä¸æº€ç‚¹ã«åŸºã¥ã„ã¦ã€å…·ä½“çš„ãªæ”¹å–„ç­–ã‚’å„ªå…ˆåº¦é †ã«5ã¤ææ¡ˆã—ã¦ãã ã•ã„",
    "max_tokens": 400,
    "temperature": 0.5
  }')
echo $SUGGESTION_RESPONSE
```

### 2. ç”»åƒè§£æï¼‹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
import requests
import json
import time

class ImageAnalysisWorkflow:
    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url
    
    def analyze_image_batch(self, image_urls, analysis_type="general"):
        """è¤‡æ•°ç”»åƒã®ä¸€æ‹¬è§£æ"""
        results = []
        
        for i, url in enumerate(image_urls):
            print(f"ç”»åƒ {i+1}/{len(image_urls)} ã‚’è§£æä¸­...")
            
            response = requests.post(
                f"{self.base_url}/ai/vision/analyze",
                json={
                    "image_url": url,
                    "task": analysis_type,
                    "options": {"confidence_threshold": 0.8}
                }
            )
            
            if response.status_code == 200:
                results.append(response.json())
            else:
                print(f"Error analyzing image {url}: {response.text}")
            
            time.sleep(1)  # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            
        return results
    
    def generate_report(self, analysis_results):
        """è§£æçµæœã‹ã‚‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        summary_prompt = f"""
        ä»¥ä¸‹ã®ç”»åƒè§£æçµæœã‚’ã‚‚ã¨ã«ã€åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„:
        
        è§£æçµæœ: {json.dumps(analysis_results, indent=2)}
        
        ãƒ¬ãƒãƒ¼ãƒˆã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„:
        1. å…¨ä½“çš„ãªå‚¾å‘
        2. æ³¨ç›®ã™ã¹ãç™ºè¦‹
        3. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        """
        
        response = requests.post(
            f"{self.base_url}/ai/llm/completion",
            json={
                "prompt": summary_prompt,
                "max_tokens": 800,
                "temperature": 0.4
            }
        )
        
        return response.json()

# ä½¿ç”¨ä¾‹
workflow = ImageAnalysisWorkflow()

image_urls = [
    "https://example.com/product1.jpg",
    "https://example.com/product2.jpg", 
    "https://example.com/product3.jpg"
]

# ç”»åƒè§£æå®Ÿè¡Œ
analysis_results = workflow.analyze_image_batch(image_urls, "product_quality")

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
report = workflow.generate_report(analysis_results)
print("=== è§£æãƒ¬ãƒãƒ¼ãƒˆ ===")
print(report["result"])
```

### 3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ

```python
import requests
import time
import json
from datetime import datetime

class AiSembleMonitor:
    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url
        self.alert_conditions = {
            "response_time_threshold": 5.0,
            "error_rate_threshold": 0.1
        }
    
    def check_health(self):
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            start_time = time.time()
            response = requests.get(f"{self.base_url}/health", timeout=10)
            response_time = time.time() - start_time
            
            return {
                "status": "healthy" if response.status_code == 200 else "unhealthy",
                "response_time": response_time,
                "timestamp": datetime.now().isoformat(),
                "details": response.json() if response.status_code == 200 else None
            }
        except Exception as e:
            return {
                "status": "error",
                "response_time": None,
                "timestamp": datetime.now().isoformat(),
                "error": str(e)
            }
    
    def check_ai_services(self):
        """AIå„ã‚µãƒ¼ãƒ“ã‚¹ã®å‹•ä½œç¢ºèª"""
        services = {
            "llm": {
                "endpoint": "/ai/llm/completion",
                "test_payload": {
                    "prompt": "Health check test",
                    "max_tokens": 10,
                    "temperature": 0.1
                }
            },
            "data_processor": {
                "endpoint": "/data/process",
                "test_payload": {
                    "operation": "analyze",
                    "data": {"records": [{"test": 1}]}
                }
            }
        }
        
        results = {}
        for service_name, config in services.items():
            try:
                start_time = time.time()
                response = requests.post(
                    f"{self.base_url}{config['endpoint']}",
                    json=config["test_payload"],
                    timeout=30
                )
                response_time = time.time() - start_time
                
                results[service_name] = {
                    "status": "healthy" if response.status_code == 200 else "unhealthy",
                    "response_time": response_time,
                    "error": None if response.status_code == 200 else response.text
                }
            except Exception as e:
                results[service_name] = {
                    "status": "error",
                    "response_time": None,
                    "error": str(e)
                }
        
        return results
    
    def run_monitoring_loop(self, interval=60):
        """ç¶™ç¶šçš„ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        print("Ai-semble v2 ç›£è¦–é–‹å§‹...")
        
        while True:
            try:
                # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
                health = self.check_health()
                print(f"[{health['timestamp']}] Health: {health['status']} ({health.get('response_time', 'N/A')}s)")
                
                # ã‚µãƒ¼ãƒ“ã‚¹ãƒã‚§ãƒƒã‚¯
                services = self.check_ai_services()
                for service, status in services.items():
                    print(f"  {service}: {status['status']} ({status.get('response_time', 'N/A')}s)")
                
                # ã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®š
                if health['status'] != 'healthy':
                    self.send_alert(f"Platform health check failed: {health}")
                
                for service, status in services.items():
                    if status['status'] != 'healthy':
                        self.send_alert(f"Service {service} is unhealthy: {status}")
                    elif status['response_time'] and status['response_time'] > self.alert_conditions['response_time_threshold']:
                        self.send_alert(f"Service {service} response time high: {status['response_time']}s")
                
                time.sleep(interval)
                
            except KeyboardInterrupt:
                print("ç›£è¦–ã‚’åœæ­¢ã—ã¾ã™...")
                break
            except Exception as e:
                print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(interval)
    
    def send_alert(self, message):
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡ï¼ˆãƒ­ã‚°å‡ºåŠ›ãƒ»å¤–éƒ¨é€šçŸ¥ï¼‰"""
        alert = {
            "timestamp": datetime.now().isoformat(),
            "level": "WARNING",
            "message": message
        }
        print(f"ğŸš¨ ALERT: {json.dumps(alert)}")
        # ã“ã“ã§å®Ÿéš›ã®é€šçŸ¥ï¼ˆSlackã€ãƒ¡ãƒ¼ãƒ«ç­‰ï¼‰ã‚’å®Ÿè£…

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    monitor = AiSembleMonitor()
    monitor.run_monitoring_loop(interval=30)  # 30ç§’é–“éš”ã§ç›£è¦–
```

## ğŸš¨ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### Pythonã§ã®å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
import requests
import time
from typing import Dict, Any, Optional

class AiSembleClient:
    def __init__(self, base_url: str = "http://localhost:8080", max_retries: int = 3):
        self.base_url = base_url
        self.max_retries = max_retries
        self.session = requests.Session()
    
    def make_request(self, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        url = f"{self.base_url}{endpoint}"
        
        for attempt in range(self.max_retries):
            try:
                response = self.session.request(method, url, timeout=30, **kwargs)
                
                if response.status_code == 200:
                    return {"success": True, "data": response.json()}
                elif response.status_code == 429:  # Rate limit
                    wait_time = 2 ** attempt
                    print(f"Rate limited. Waiting {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code >= 500:  # Server error
                    if attempt < self.max_retries - 1:
                        wait_time = 2 ** attempt
                        print(f"Server error. Retrying in {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return {
                            "success": False,
                            "error": f"Server error: {response.status_code}",
                            "details": response.text
                        }
                else:
                    return {
                        "success": False,
                        "error": f"Client error: {response.status_code}",
                        "details": response.text
                    }
                    
            except requests.exceptions.ConnectionError:
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"Connection error. Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    return {
                        "success": False,
                        "error": "Connection failed after retries"
                    }
            except requests.exceptions.Timeout:
                return {
                    "success": False,
                    "error": "Request timeout"
                }
            except Exception as e:
                return {
                    "success": False,
                    "error": f"Unexpected error: {str(e)}"
                }
        
        return {"success": False, "error": "Max retries exceeded"}
    
    def llm_completion(self, prompt: str, **options) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãLLMæ¨è«–"""
        payload = {"prompt": prompt, **options}
        result = self.make_request("POST", "/ai/llm/completion", json=payload)
        
        if not result["success"]:
            print(f"LLM completion failed: {result['error']}")
            return result
        
        # ã‚¸ãƒ§ãƒ–IDãŒã‚ã‚‹å ´åˆã¯çµæœã‚’å¾…æ©Ÿ
        job_id = result["data"].get("job_id")
        if job_id and result["data"].get("status") == "pending":
            return self.wait_for_job_completion(job_id)
        
        return result
    
    def wait_for_job_completion(self, job_id: str, timeout: int = 300) -> Dict[str, Any]:
        """ã‚¸ãƒ§ãƒ–å®Œäº†ã‚’å¾…æ©Ÿ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            result = self.make_request("GET", f"/jobs/{job_id}")
            
            if not result["success"]:
                return result
            
            status = result["data"]["status"]
            if status == "completed":
                return {"success": True, "data": result["data"]}
            elif status == "failed":
                return {
                    "success": False,
                    "error": "Job failed",
                    "details": result["data"].get("error")
                }
            
            time.sleep(2)  # 2ç§’é–“éš”ã§ãƒãƒ¼ãƒªãƒ³ã‚°
        
        return {"success": False, "error": "Job timeout"}

# ä½¿ç”¨ä¾‹
client = AiSembleClient()

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã§LLMå®Ÿè¡Œ
result = client.llm_completion(
    "Pythonã®ç‰¹å¾´ã‚’èª¬æ˜ã—ã¦ãã ã•ã„",
    max_tokens=200,
    temperature=0.7
)

if result["success"]:
    print("æˆåŠŸ:", result["data"]["result"])
else:
    print("ã‚¨ãƒ©ãƒ¼:", result["error"])
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–

```python
import asyncio
import aiohttp
from typing import List, Dict, Any

class AsyncAiSembleClient:
    def __init__(self, base_url: str = "http://localhost:8080", max_concurrent: int = 5):
        self.base_url = base_url
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def make_async_request(self, session: aiohttp.ClientSession, method: str, endpoint: str, **kwargs) -> Dict[str, Any]:
        """éåŒæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ"""
        async with self.semaphore:
            try:
                url = f"{self.base_url}{endpoint}"
                async with session.request(method, url, **kwargs) as response:
                    if response.status == 200:
                        data = await response.json()
                        return {"success": True, "data": data}
                    else:
                        text = await response.text()
                        return {
                            "success": False,
                            "error": f"HTTP {response.status}",
                            "details": text
                        }
            except Exception as e:
                return {"success": False, "error": str(e)}
    
    async def batch_llm_completion(self, prompts: List[str], **options) -> List[Dict[str, Any]]:
        """è¤‡æ•°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸¦åˆ—å‡¦ç†"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for prompt in prompts:
                payload = {"prompt": prompt, **options}
                task = self.make_async_request(
                    session, "POST", "/ai/llm/completion", json=payload
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            return results
    
    async def batch_data_processing(self, operations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for operation in operations:
                task = self.make_async_request(
                    session, "POST", "/data/process", json=operation
                )
                tasks.append(task)
            
            results = await asyncio.gather(*tasks)
            return results

# ä½¿ç”¨ä¾‹
async def main():
    client = AsyncAiSembleClient(max_concurrent=3)
    
    # è¤‡æ•°ã®LLMæ¨è«–ã‚’ä¸¦åˆ—å®Ÿè¡Œ
    prompts = [
        "AIã®æ­´å²ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "æ©Ÿæ¢°å­¦ç¿’ã¨ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®é•ã„ã¯ï¼Ÿ",
        "è‡ªç„¶è¨€èªå‡¦ç†ã®å¿œç”¨ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„"
    ]
    
    results = await client.batch_llm_completion(
        prompts,
        max_tokens=150,
        temperature=0.7
    )
    
    for i, result in enumerate(results):
        if result["success"]:
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1}: {result['data']['result'][:100]}...")
        else:
            print(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1} ã‚¨ãƒ©ãƒ¼: {result['error']}")

# å®Ÿè¡Œ
asyncio.run(main())
```

## ğŸ“š SDKãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ä¾‹

### ã‚«ã‚¹ã‚¿ãƒ SDKã‚¯ãƒ©ã‚¹

```python
# ai_semble_sdk.py
import requests
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass
class JobResult:
    job_id: str
    status: str
    result: Optional[Any] = None
    error: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

class AiSembleSDK:
    """Ai-semble v2 å…¬å¼SDK"""
    
    def __init__(self, base_url: str = "http://localhost:8080", api_key: Optional[str] = None):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        
        if api_key:
            self.session.headers.update({"Authorization": f"Bearer {api_key}"})
    
    def health(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        response = self.session.get(f"{self.base_url}/health")
        response.raise_for_status()
        return response.json()
    
    def llm_completion(self, prompt: str, model: str = "default", 
                      max_tokens: int = 1000, temperature: float = 0.7) -> JobResult:
        """LLMæ¨è«–å®Ÿè¡Œ"""
        payload = {
            "prompt": prompt,
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        response = self.session.post(f"{self.base_url}/ai/llm/completion", json=payload)
        response.raise_for_status()
        data = response.json()
        
        return JobResult(
            job_id=data.get("job_id", ""),
            status=data.get("status", ""),
            result=data.get("result"),
            error=data.get("error")
        )
    
    def vision_analyze(self, image_url: str, task: str = "analyze", 
                      options: Optional[Dict[str, Any]] = None) -> JobResult:
        """ç”»åƒè§£æå®Ÿè¡Œ"""
        payload = {
            "image_url": image_url,
            "task": task,
            "options": options or {}
        }
        
        response = self.session.post(f"{self.base_url}/ai/vision/analyze", json=payload)
        response.raise_for_status()
        data = response.json()
        
        return JobResult(
            job_id=data.get("job_id", ""),
            status=data.get("status", ""),
            result=data.get("result"),
            error=data.get("error")
        )
    
    def process_data(self, operation: str, data: Dict[str, Any], 
                    options: Optional[Dict[str, Any]] = None) -> JobResult:
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Ÿè¡Œ"""
        payload = {
            "operation": operation,
            "data": data,
            "options": options or {}
        }
        
        response = self.session.post(f"{self.base_url}/data/process", json=payload)
        response.raise_for_status()
        result_data = response.json()
        
        return JobResult(
            job_id="data-" + str(hash(json.dumps(payload))),
            status=result_data.get("status", ""),
            result=result_data.get("result"),
            error=result_data.get("error")
        )
    
    def get_job_status(self, job_id: str) -> JobResult:
        """ã‚¸ãƒ§ãƒ–çŠ¶æ…‹å–å¾—"""
        response = self.session.get(f"{self.base_url}/jobs/{job_id}")
        response.raise_for_status()
        data = response.json()
        
        return JobResult(
            job_id=data.get("job_id", job_id),
            status=data.get("status", ""),
            result=data.get("result"),
            error=data.get("error"),
            created_at=datetime.fromisoformat(data["created_at"]) if data.get("created_at") else None,
            updated_at=datetime.fromisoformat(data["updated_at"]) if data.get("updated_at") else None
        )
    
    def upload_file(self, file_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = self.session.post(f"{self.base_url}/data/upload", files=files)
        
        response.raise_for_status()
        return response.json()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # SDKåˆæœŸåŒ–
    sdk = AiSembleSDK()
    
    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    health = sdk.health()
    print(f"Platform status: {health['status']}")
    
    # LLMæ¨è«–
    llm_result = sdk.llm_completion(
        prompt="Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
        max_tokens=200,
        temperature=0.5
    )
    print(f"LLM Result: {llm_result.result}")
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    data_result = sdk.process_data(
        operation="analyze",
        data={
            "records": [
                {"name": "Alice", "score": 85},
                {"name": "Bob", "score": 92}
            ]
        }
    )
    print(f"Data Analysis: {data_result.result}")
```

ã“ã‚Œã‚‰ã®ä¾‹ã«ã‚ˆã‚Šã€Ai-semble v2ã®å…·ä½“çš„ãªä½¿ç”¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã¨å®Ÿè£…æ–¹æ³•ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚